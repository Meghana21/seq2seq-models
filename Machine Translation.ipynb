{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement - Translate English Statements to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re # to work with regular expressions\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the data and observing the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_Text</th>\n",
       "      <th>Hindi_Text</th>\n",
       "      <th>Extra_Info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Wow!</td>\n",
       "      <td>वाह!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Help!</td>\n",
       "      <td>बचाओ!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>उछलो.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>कूदो.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>छलांग.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Hello!</td>\n",
       "      <td>नमस्ते।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Hello!</td>\n",
       "      <td>नमस्कार।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Cheers!</td>\n",
       "      <td>वाह-वाह!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Cheers!</td>\n",
       "      <td>चियर्स!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Got it?</td>\n",
       "      <td>समझे कि नहीं?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>I'm OK.</td>\n",
       "      <td>मैं ठीक हूँ।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Awesome!</td>\n",
       "      <td>बहुत बढ़िया!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Come in.</td>\n",
       "      <td>अंदर आ जाओ।</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Get out!</td>\n",
       "      <td>बाहर निकल जाओ!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Go away!</td>\n",
       "      <td>चले जाओ!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English_Text      Hindi_Text  \\\n",
       "0          Wow!            वाह!   \n",
       "1         Help!           बचाओ!   \n",
       "2         Jump.           उछलो.   \n",
       "3         Jump.           कूदो.   \n",
       "4         Jump.          छलांग.   \n",
       "5        Hello!         नमस्ते।   \n",
       "6        Hello!        नमस्कार।   \n",
       "7       Cheers!        वाह-वाह!   \n",
       "8       Cheers!         चियर्स!   \n",
       "9       Got it?   समझे कि नहीं?   \n",
       "10      I'm OK.    मैं ठीक हूँ।   \n",
       "11     Awesome!    बहुत बढ़िया!   \n",
       "12     Come in.     अंदर आ जाओ।   \n",
       "13     Get out!  बाहर निकल जाओ!   \n",
       "14     Go away!        चले जाओ!   \n",
       "\n",
       "                                           Extra_Info  \n",
       "0   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  \n",
       "1   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "2   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "3   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "4   CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "5   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "6   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "7   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "8   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "9   CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "10  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "11  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "12  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "13  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "14  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('hin.txt',sep='\\t',header=None)\n",
    "data.columns=[\"English_Text\",\"Hindi_Text\",\"Extra_Info\"]\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the Extra info column \n",
    "data.drop(\"Extra_Info\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_Text</th>\n",
       "      <th>Hindi_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Wow!</td>\n",
       "      <td>वाह!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Help!</td>\n",
       "      <td>बचाओ!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>उछलो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>कूदो.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Jump.</td>\n",
       "      <td>छलांग.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English_Text Hindi_Text\n",
       "0         Wow!       वाह!\n",
       "1        Help!      बचाओ!\n",
       "2        Jump.      उछलो.\n",
       "3        Jump.      कूदो.\n",
       "4        Jump.     छलांग."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lower all characters\n",
    "2. Remove special characters\n",
    "3. Remove extra spaces\n",
    "4. Remove quotes\n",
    "5. Remove all numbers from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['English_Text'] = data['English_Text'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string.punctuation will give the all sets of punctuation, we need to exclude these from the data\n",
    "remove = set(string.punctuation)\n",
    "\n",
    "data['English_Text'] = data['English_Text'].apply(lambda x:''.join([ele for ele in x if ele not in remove]))\n",
    "data['Hindi_Text'] = data['Hindi_Text'].apply(lambda x:''.join([ele for ele in x if ele not in remove]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() method removes any leading trailing spaces \n",
    "data['English_Text']=data['English_Text'].apply(lambda x:x.strip())\n",
    "data['Hindi_Text']=data['Hindi_Text'].apply(lambda x:x.strip())\n",
    "\n",
    "# Replace all extra-spaces and replacing them with single space using re.sub(what to replace,replacewith) \n",
    "# regex syntax for extra space \" +\" \n",
    "data['English_Text'] = data['English_Text'].apply(lambda x: re.sub(\" +\",\" \",x))\n",
    "data['Hindi_Text']=data['Hindi_Text'].apply(lambda x: re.sub(\" +\",\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes\n",
    "data['English_Text'] = data['English_Text'].apply(lambda x: re.sub(\"'\",'',x))\n",
    "data['Hindi_Text']=data['Hindi_Text'].apply(lambda x: re.sub(\"'\",'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping table (dictionary -----> remove), and use it in the translate() method \n",
    "# this will remove all the digits in a scentence\n",
    "digits= '0123456789'\n",
    "remove=str.maketrans('', '', digits)\n",
    "\n",
    "data['English_Text'] = data['English_Text'].apply(lambda x: x.translate(remove))\n",
    "data['Hindi_Text'] = data['Hindi_Text'].apply(lambda x: x.translate(remove))\n",
    "\n",
    "# removing hindi numbers as well, if any\n",
    "data['Hindi_Text'] = data['Hindi_Text'].apply(lambda x: re.sub('[२३०८१५७९४६]','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add START_ and _END to the Target (Hindi) sentences and creating vocabulary of unique English and Hindi words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hindi_Text'] = data['Hindi_Text'].apply(lambda x : 'START_ '+ x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English and Hindi Vocabulary - we are taking words from the training set itself\n",
    "english_words = set()\n",
    "for ele in data['English_Text']:\n",
    "    for word in ele.split():\n",
    "        english_words.add(word)\n",
    "        \n",
    "        \n",
    "hindi_words = set()\n",
    "for ele in data['Hindi_Text']:\n",
    "    for word in ele.split():\n",
    "        hindi_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343\n",
      "2969\n"
     ]
    }
   ],
   "source": [
    "# vocabulary sizes \n",
    "print(len(english_words))\n",
    "print(len(hindi_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Now comes the very interesting TOKENISATION step - every word in the input sentences of the training set is into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a neural network to predict on text data, it first has to be turned into data it can understand. \n",
    "\n",
    "\n",
    "Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1. We can turn each character into a number or each word into a number. These are called character and word level embeddings, respectively. \n",
    "\n",
    "\n",
    "2. Character embeddings are used for character level models that generate text predictions for each character.\n",
    "\n",
    "\n",
    "3. A word level model uses word embeddings that generate text predictions for each word. \n",
    "\n",
    "\n",
    "4. Word level models tend to learn better, since they are lower in complexity, so we'll use those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisation - in random order\n",
    "# Turn each sentence into a sequence of words embeddings using Keras's Tokenizer function. \n",
    "# Using this function to tokenize Engilsh and Hindi scentences in the cell below.\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    x: List of sentences/strings to be tokenized\n",
    "    returns : Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \n",
    "    \"\"\"\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    t=tokenizer.texts_to_sequences(x)\n",
    "    return t, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'a': 2, 'her': 3, 'meghana': 4, 'good': 5, 'executor': 6, 'she': 7, 'constant': 8, 'learner': 9, 'and': 10, 'also': 11, 'implements': 12, 'learnings': 13, 'this': 14, 'made': 15, 'an': 16, 'excellent': 17, 'problem': 18, 'solver': 19}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  Meghana is a good executor.\n",
      "  Output: [4, 1, 2, 5, 6]\n",
      "Sequence 2 in x\n",
      "  Input:  She is a constant learner and also implements her learnings.\n",
      "  Output: [7, 1, 2, 8, 9, 10, 11, 12, 3, 13]\n",
      "Sequence 3 in x\n",
      "  Input:  This made her an excellent problem-solver.\n",
      "  Output: [14, 15, 3, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "# testing the tokenizer on an input scentence\n",
    "text_sentences = [\n",
    "    'Meghana is a good executor.',\n",
    "    'She is a constant learner and also implements her learnings.',\n",
    "    'This made her an excellent problem-solver.']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2343, 2970)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# other way of tokenisation - in a sorted order\n",
    "input_words = sorted(list(english_words))\n",
    "target_words = sorted(list(hindi_words))\n",
    "\n",
    "num_encoder_tokens = len(input_words)\n",
    "num_decoder_tokens = len(target_words)+1\n",
    "\n",
    "num_encoder_tokens,num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 Python dictionaries to convert a given word into an integer index and \n",
    "input_token_index =  dict([(ele,i+1) for i,ele in enumerate(input_words)])\n",
    "target_token_index = dict([(ele,i+1) for i,ele in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'abandoned': 2,\n",
       " 'ability': 3,\n",
       " 'ablaze': 4,\n",
       " 'able': 5,\n",
       " 'about': 6,\n",
       " 'above': 7,\n",
       " 'abroad': 8,\n",
       " 'absence': 9,\n",
       " 'absent': 10,\n",
       " 'absolute': 11,\n",
       " 'absurd': 12,\n",
       " 'abused': 13,\n",
       " 'accepted': 14,\n",
       " 'access': 15,\n",
       " 'accident': 16,\n",
       " 'accidental': 17,\n",
       " 'accompanied': 18,\n",
       " 'accompany': 19,\n",
       " 'according': 20,\n",
       " 'account': 21,\n",
       " 'accountable': 22,\n",
       " 'accused': 23,\n",
       " 'accustomed': 24,\n",
       " 'ache': 25,\n",
       " 'acknowledgement': 26,\n",
       " 'acquaintance': 27,\n",
       " 'acquaintances': 28,\n",
       " 'acquainted': 29,\n",
       " 'across': 30,\n",
       " 'act': 31,\n",
       " 'actions': 32,\n",
       " 'actor': 33,\n",
       " 'actress': 34,\n",
       " 'add': 35,\n",
       " 'adding': 36,\n",
       " 'address': 37,\n",
       " 'admit': 38,\n",
       " 'adopted': 39,\n",
       " 'advantage': 40,\n",
       " 'advice': 41,\n",
       " 'advise': 42,\n",
       " 'advised': 43,\n",
       " 'affected': 44,\n",
       " 'afford': 45,\n",
       " 'afraid': 46,\n",
       " 'africa': 47,\n",
       " 'after': 48,\n",
       " 'afternoon': 49,\n",
       " 'again': 50,\n",
       " 'against': 51,\n",
       " 'age': 52,\n",
       " 'ago': 53,\n",
       " 'agree': 54,\n",
       " 'agreement': 55,\n",
       " 'aids': 56,\n",
       " 'air': 57,\n",
       " 'airport': 58,\n",
       " 'alarm': 59,\n",
       " 'alcohol': 60,\n",
       " 'alike': 61,\n",
       " 'alive': 62,\n",
       " 'all': 63,\n",
       " 'allergic': 64,\n",
       " 'allow': 65,\n",
       " 'allowances': 66,\n",
       " 'allowed': 67,\n",
       " 'almost': 68,\n",
       " 'alone': 69,\n",
       " 'along': 70,\n",
       " 'aloud': 71,\n",
       " 'alphabet': 72,\n",
       " 'already': 73,\n",
       " 'also': 74,\n",
       " 'although': 75,\n",
       " 'always': 76,\n",
       " 'am': 77,\n",
       " 'amateur': 78,\n",
       " 'amazed': 79,\n",
       " 'ambitions': 80,\n",
       " 'ambulance': 81,\n",
       " 'amend': 82,\n",
       " 'america': 83,\n",
       " 'american': 84,\n",
       " 'among': 85,\n",
       " 'amount': 86,\n",
       " 'an': 87,\n",
       " 'anchorage': 88,\n",
       " 'and': 89,\n",
       " 'angels': 90,\n",
       " 'angles': 91,\n",
       " 'angry': 92,\n",
       " 'animal': 93,\n",
       " 'animals': 94,\n",
       " 'annoys': 95,\n",
       " 'another': 96,\n",
       " 'answer': 97,\n",
       " 'answered': 98,\n",
       " 'answering': 99,\n",
       " 'answers': 100,\n",
       " 'anxious': 101,\n",
       " 'any': 102,\n",
       " 'anybody': 103,\n",
       " 'anymore': 104,\n",
       " 'anyone': 105,\n",
       " 'anything': 106,\n",
       " 'anywhere': 107,\n",
       " 'apart': 108,\n",
       " 'apartment': 109,\n",
       " 'apology': 110,\n",
       " 'appearance': 111,\n",
       " 'appeared': 112,\n",
       " 'appearing': 113,\n",
       " 'appears': 114,\n",
       " 'apple': 115,\n",
       " 'apples': 116,\n",
       " 'applies': 117,\n",
       " 'appointment': 118,\n",
       " 'approached': 119,\n",
       " 'april': 120,\n",
       " 'arabic': 121,\n",
       " 'are': 122,\n",
       " 'arent': 123,\n",
       " 'argued': 124,\n",
       " 'arguing': 125,\n",
       " 'arms': 126,\n",
       " 'around': 127,\n",
       " 'arrange': 128,\n",
       " 'arrest': 129,\n",
       " 'arrested': 130,\n",
       " 'arrive': 131,\n",
       " 'arrived': 132,\n",
       " 'arrogance': 133,\n",
       " 'arrogant': 134,\n",
       " 'art': 135,\n",
       " 'artist': 136,\n",
       " 'as': 137,\n",
       " 'ashamed': 138,\n",
       " 'asia': 139,\n",
       " 'ask': 140,\n",
       " 'asked': 141,\n",
       " 'asking': 142,\n",
       " 'asleep': 143,\n",
       " 'assaulted': 144,\n",
       " 'at': 145,\n",
       " 'atheist': 146,\n",
       " 'atmosphere': 147,\n",
       " 'attend': 148,\n",
       " 'attended': 149,\n",
       " 'attention': 150,\n",
       " 'august': 151,\n",
       " 'aunt': 152,\n",
       " 'australia': 153,\n",
       " 'author': 154,\n",
       " 'authorities': 155,\n",
       " 'awake': 156,\n",
       " 'aware': 157,\n",
       " 'away': 158,\n",
       " 'awesome': 159,\n",
       " 'b': 160,\n",
       " 'baby': 161,\n",
       " 'back': 162,\n",
       " 'backs': 163,\n",
       " 'bad': 164,\n",
       " 'bag': 165,\n",
       " 'baggage': 166,\n",
       " 'bags': 167,\n",
       " 'bakery': 168,\n",
       " 'balance': 169,\n",
       " 'ball': 170,\n",
       " 'balls': 171,\n",
       " 'bank': 172,\n",
       " 'bankruptcy': 173,\n",
       " 'bar': 174,\n",
       " 'barbers': 175,\n",
       " 'barely': 176,\n",
       " 'barking': 177,\n",
       " 'baseball': 178,\n",
       " 'basketball': 179,\n",
       " 'bat': 180,\n",
       " 'bath': 181,\n",
       " 'bathroom': 182,\n",
       " 'batter': 183,\n",
       " 'be': 184,\n",
       " 'beach': 185,\n",
       " 'bear': 186,\n",
       " 'beard': 187,\n",
       " 'beast': 188,\n",
       " 'beating': 189,\n",
       " 'beatles': 190,\n",
       " 'beautiful': 191,\n",
       " 'beautifully': 192,\n",
       " 'beauty': 193,\n",
       " 'became': 194,\n",
       " 'because': 195,\n",
       " 'become': 196,\n",
       " 'becoming': 197,\n",
       " 'bed': 198,\n",
       " 'been': 199,\n",
       " 'beer': 200,\n",
       " 'before': 201,\n",
       " 'beforehand': 202,\n",
       " 'beg': 203,\n",
       " 'began': 204,\n",
       " 'begin': 205,\n",
       " 'beginning': 206,\n",
       " 'beginnings': 207,\n",
       " 'begins': 208,\n",
       " 'begun': 209,\n",
       " 'behavior': 210,\n",
       " 'behind': 211,\n",
       " 'being': 212,\n",
       " 'believe': 213,\n",
       " 'believes': 214,\n",
       " 'bell': 215,\n",
       " 'bent': 216,\n",
       " 'bern': 217,\n",
       " 'besides': 218,\n",
       " 'best': 219,\n",
       " 'bet': 220,\n",
       " 'betrayed': 221,\n",
       " 'better': 222,\n",
       " 'between': 223,\n",
       " 'bicycle': 224,\n",
       " 'big': 225,\n",
       " 'bigger': 226,\n",
       " 'biggest': 227,\n",
       " 'bill': 228,\n",
       " 'billion': 229,\n",
       " 'biology': 230,\n",
       " 'bird': 231,\n",
       " 'birds': 232,\n",
       " 'birthday': 233,\n",
       " 'bit': 234,\n",
       " 'bite': 235,\n",
       " 'bitter': 236,\n",
       " 'biwa': 237,\n",
       " 'black': 238,\n",
       " 'blame': 239,\n",
       " 'blanket': 240,\n",
       " 'bloody': 241,\n",
       " 'bloom': 242,\n",
       " 'blowing': 243,\n",
       " 'blue': 244,\n",
       " 'board': 245,\n",
       " 'boat': 246,\n",
       " 'bombay': 247,\n",
       " 'bone': 248,\n",
       " 'bones': 249,\n",
       " 'book': 250,\n",
       " 'books': 251,\n",
       " 'booted': 252,\n",
       " 'bored': 253,\n",
       " 'born': 254,\n",
       " 'borrow': 255,\n",
       " 'boss': 256,\n",
       " 'boston': 257,\n",
       " 'both': 258,\n",
       " 'bother': 259,\n",
       " 'bottles': 260,\n",
       " 'bought': 261,\n",
       " 'bounces': 262,\n",
       " 'bound': 263,\n",
       " 'bowls': 264,\n",
       " 'box': 265,\n",
       " 'boy': 266,\n",
       " 'boys': 267,\n",
       " 'branches': 268,\n",
       " 'brave': 269,\n",
       " 'bread': 270,\n",
       " 'break': 271,\n",
       " 'breakfast': 272,\n",
       " 'breath': 273,\n",
       " 'breathe': 274,\n",
       " 'breathed': 275,\n",
       " 'bride': 276,\n",
       " 'bridge': 277,\n",
       " 'bring': 278,\n",
       " 'britain': 279,\n",
       " 'british': 280,\n",
       " 'broke': 281,\n",
       " 'broken': 282,\n",
       " 'brother': 283,\n",
       " 'brothers': 284,\n",
       " 'brought': 285,\n",
       " 'brown': 286,\n",
       " 'brush': 287,\n",
       " 'buddhism': 288,\n",
       " 'bugs': 289,\n",
       " 'build': 290,\n",
       " 'buildings': 291,\n",
       " 'bullet': 292,\n",
       " 'bum': 293,\n",
       " 'bun': 294,\n",
       " 'burglar': 295,\n",
       " 'buried': 296,\n",
       " 'burned': 297,\n",
       " 'burning': 298,\n",
       " 'burns': 299,\n",
       " 'burst': 300,\n",
       " 'bus': 301,\n",
       " 'bush': 302,\n",
       " 'business': 303,\n",
       " 'busy': 304,\n",
       " 'but': 305,\n",
       " 'butter': 306,\n",
       " 'buy': 307,\n",
       " 'by': 308,\n",
       " 'cafe': 309,\n",
       " 'cafeteria': 310,\n",
       " 'cage': 311,\n",
       " 'cake': 312,\n",
       " 'calculation': 313,\n",
       " 'calcutta': 314,\n",
       " 'call': 315,\n",
       " 'called': 316,\n",
       " 'calls': 317,\n",
       " 'came': 318,\n",
       " 'camera': 319,\n",
       " 'can': 320,\n",
       " 'canada': 321,\n",
       " 'cancer': 322,\n",
       " 'cannot': 323,\n",
       " 'cant': 324,\n",
       " 'capital': 325,\n",
       " 'car': 326,\n",
       " 'care': 327,\n",
       " 'careful': 328,\n",
       " 'carefully': 329,\n",
       " 'careless': 330,\n",
       " 'carelessness': 331,\n",
       " 'carried': 332,\n",
       " 'carries': 333,\n",
       " 'cars': 334,\n",
       " 'case': 335,\n",
       " 'cash': 336,\n",
       " 'cat': 337,\n",
       " 'catch': 338,\n",
       " 'catches': 339,\n",
       " 'catching': 340,\n",
       " 'catholic': 341,\n",
       " 'cats': 342,\n",
       " 'caught': 343,\n",
       " 'caused': 344,\n",
       " 'causes': 345,\n",
       " 'cave': 346,\n",
       " 'cease': 347,\n",
       " 'ceased': 348,\n",
       " 'ceiling': 349,\n",
       " 'celebrated': 350,\n",
       " 'cemetery': 351,\n",
       " 'century': 352,\n",
       " 'certain': 353,\n",
       " 'chain': 354,\n",
       " 'chair': 355,\n",
       " 'chairs': 356,\n",
       " 'chance': 357,\n",
       " 'changed': 358,\n",
       " 'changing': 359,\n",
       " 'character': 360,\n",
       " 'charge': 361,\n",
       " 'charged': 362,\n",
       " 'chat': 363,\n",
       " 'chatting': 364,\n",
       " 'cheaper': 365,\n",
       " 'cheek': 366,\n",
       " 'cheers': 367,\n",
       " 'cheese': 368,\n",
       " 'child': 369,\n",
       " 'childhood': 370,\n",
       " 'children': 371,\n",
       " 'china': 372,\n",
       " 'chinese': 373,\n",
       " 'choose': 374,\n",
       " 'chose': 375,\n",
       " 'church': 376,\n",
       " 'cities': 377,\n",
       " 'citizen': 378,\n",
       " 'city': 379,\n",
       " 'class': 380,\n",
       " 'classical': 381,\n",
       " 'classmates': 382,\n",
       " 'classroom': 383,\n",
       " 'clean': 384,\n",
       " 'cleaned': 385,\n",
       " 'cleaning': 386,\n",
       " 'clear': 387,\n",
       " 'cleared': 388,\n",
       " 'clever': 389,\n",
       " 'cliff': 390,\n",
       " 'climate': 391,\n",
       " 'climb': 392,\n",
       " 'climbed': 393,\n",
       " 'clock': 394,\n",
       " 'close': 395,\n",
       " 'closed': 396,\n",
       " 'closing': 397,\n",
       " 'cloth': 398,\n",
       " 'clothes': 399,\n",
       " 'cloud': 400,\n",
       " 'clouds': 401,\n",
       " 'club': 402,\n",
       " 'clung': 403,\n",
       " 'coal': 404,\n",
       " 'coffee': 405,\n",
       " 'coincidentally': 406,\n",
       " 'coins': 407,\n",
       " 'cold': 408,\n",
       " 'colds': 409,\n",
       " 'collecting': 410,\n",
       " 'college': 411,\n",
       " 'color': 412,\n",
       " 'colorful': 413,\n",
       " 'columbus': 414,\n",
       " 'come': 415,\n",
       " 'comes': 416,\n",
       " 'comfort': 417,\n",
       " 'coming': 418,\n",
       " 'command': 419,\n",
       " 'commit': 420,\n",
       " 'committed': 421,\n",
       " 'committee': 422,\n",
       " 'common': 423,\n",
       " 'communicate': 424,\n",
       " 'companies': 425,\n",
       " 'company': 426,\n",
       " 'complained': 427,\n",
       " 'complaining': 428,\n",
       " 'complains': 429,\n",
       " 'completed': 430,\n",
       " 'completely': 431,\n",
       " 'comprehend': 432,\n",
       " 'computer': 433,\n",
       " 'concealed': 434,\n",
       " 'concern': 435,\n",
       " 'concert': 436,\n",
       " 'condition': 437,\n",
       " 'conditioner': 438,\n",
       " 'conduct': 439,\n",
       " 'conference': 440,\n",
       " 'confident': 441,\n",
       " 'conform': 442,\n",
       " 'congratulate': 443,\n",
       " 'congratulations': 444,\n",
       " 'consciousness': 445,\n",
       " 'consented': 446,\n",
       " 'consider': 447,\n",
       " 'consists': 448,\n",
       " 'constantly': 449,\n",
       " 'constitution': 450,\n",
       " 'contact': 451,\n",
       " 'contain': 452,\n",
       " 'contains': 453,\n",
       " 'continent': 454,\n",
       " 'continued': 455,\n",
       " 'contracted': 456,\n",
       " 'control': 457,\n",
       " 'convinced': 458,\n",
       " 'cook': 459,\n",
       " 'cooked': 460,\n",
       " 'cooking': 461,\n",
       " 'copies': 462,\n",
       " 'copyrighted': 463,\n",
       " 'corner': 464,\n",
       " 'correct': 465,\n",
       " 'corruption': 466,\n",
       " 'cost': 467,\n",
       " 'costly': 468,\n",
       " 'coughing': 469,\n",
       " 'could': 470,\n",
       " 'couldnt': 471,\n",
       " 'count': 472,\n",
       " 'counting': 473,\n",
       " 'countries': 474,\n",
       " 'country': 475,\n",
       " 'countryside': 476,\n",
       " 'couple': 477,\n",
       " 'courage': 478,\n",
       " 'cousin': 479,\n",
       " 'cover': 480,\n",
       " 'covered': 481,\n",
       " 'cows': 482,\n",
       " 'cradle': 483,\n",
       " 'crash': 484,\n",
       " 'cricket': 485,\n",
       " 'cried': 486,\n",
       " 'crime': 487,\n",
       " 'cross': 488,\n",
       " 'crossing': 489,\n",
       " 'crow': 490,\n",
       " 'crows': 491,\n",
       " 'crushed': 492,\n",
       " 'cry': 493,\n",
       " 'crying': 494,\n",
       " 'cup': 495,\n",
       " 'cups': 496,\n",
       " 'cured': 497,\n",
       " 'curing': 498,\n",
       " 'current': 499,\n",
       " 'cursed': 500,\n",
       " 'customers': 501,\n",
       " 'cut': 502,\n",
       " 'cute': 503,\n",
       " 'cycling': 504,\n",
       " 'dad': 505,\n",
       " 'daily': 506,\n",
       " 'damp': 507,\n",
       " 'dance': 508,\n",
       " 'danger': 509,\n",
       " 'dangerous': 510,\n",
       " 'dark': 511,\n",
       " 'daughter': 512,\n",
       " 'daughters': 513,\n",
       " 'day': 514,\n",
       " 'days': 515,\n",
       " 'daytime': 516,\n",
       " 'dead': 517,\n",
       " 'deals': 518,\n",
       " 'dear': 519,\n",
       " 'death': 520,\n",
       " 'debt': 521,\n",
       " 'debts': 522,\n",
       " 'decay': 523,\n",
       " 'deceive': 524,\n",
       " 'decide': 525,\n",
       " 'decided': 526,\n",
       " 'decision': 527,\n",
       " 'decline': 528,\n",
       " 'declined': 529,\n",
       " 'deep': 530,\n",
       " 'deeply': 531,\n",
       " 'deer': 532,\n",
       " 'deers': 533,\n",
       " 'defeated': 534,\n",
       " 'definite': 535,\n",
       " 'definitely': 536,\n",
       " 'degree': 537,\n",
       " 'delhi': 538,\n",
       " 'deliberately': 539,\n",
       " 'demanded': 540,\n",
       " 'democracy': 541,\n",
       " 'dentist': 542,\n",
       " 'depends': 543,\n",
       " 'deprived': 544,\n",
       " 'desk': 545,\n",
       " 'destination': 546,\n",
       " 'destined': 547,\n",
       " 'destroyed': 548,\n",
       " 'detail': 549,\n",
       " 'developed': 550,\n",
       " 'devote': 551,\n",
       " 'dialed': 552,\n",
       " 'diamond': 553,\n",
       " 'diamonds': 554,\n",
       " 'dictionary': 555,\n",
       " 'did': 556,\n",
       " 'didnt': 557,\n",
       " 'die': 558,\n",
       " 'died': 559,\n",
       " 'dies': 560,\n",
       " 'differ': 561,\n",
       " 'difference': 562,\n",
       " 'different': 563,\n",
       " 'difficult': 564,\n",
       " 'digging': 565,\n",
       " 'dine': 566,\n",
       " 'dinner': 567,\n",
       " 'directions': 568,\n",
       " 'dirty': 569,\n",
       " 'disappointed': 570,\n",
       " 'disappointment': 571,\n",
       " 'discovered': 572,\n",
       " 'discussed': 573,\n",
       " 'discussing': 574,\n",
       " 'dishes': 575,\n",
       " 'dislike': 576,\n",
       " 'dissatisfied': 577,\n",
       " 'distance': 578,\n",
       " 'distances': 579,\n",
       " 'distinguish': 580,\n",
       " 'distributed': 581,\n",
       " 'disturbed': 582,\n",
       " 'disturbing': 583,\n",
       " 'do': 584,\n",
       " 'doctor': 585,\n",
       " 'does': 586,\n",
       " 'doesnt': 587,\n",
       " 'dog': 588,\n",
       " 'dogs': 589,\n",
       " 'doing': 590,\n",
       " 'doll': 591,\n",
       " 'dollars': 592,\n",
       " 'done': 593,\n",
       " 'dont': 594,\n",
       " 'door': 595,\n",
       " 'doubt': 596,\n",
       " 'dove': 597,\n",
       " 'down': 598,\n",
       " 'dozen': 599,\n",
       " 'dozens': 600,\n",
       " 'drastic': 601,\n",
       " 'drawers': 602,\n",
       " 'dream': 603,\n",
       " 'dreamed': 604,\n",
       " 'dreams': 605,\n",
       " 'drenched': 606,\n",
       " 'dress': 607,\n",
       " 'dressed': 608,\n",
       " 'dried': 609,\n",
       " 'drink': 610,\n",
       " 'drinkable': 611,\n",
       " 'drinking': 612,\n",
       " 'drinks': 613,\n",
       " 'drive': 614,\n",
       " 'driven': 615,\n",
       " 'drivers': 616,\n",
       " 'driving': 617,\n",
       " 'drop': 618,\n",
       " 'dropped': 619,\n",
       " 'drove': 620,\n",
       " 'drug': 621,\n",
       " 'due': 622,\n",
       " 'dues': 623,\n",
       " 'dug': 624,\n",
       " 'during': 625,\n",
       " 'dust': 626,\n",
       " 'each': 627,\n",
       " 'eager': 628,\n",
       " 'ear': 629,\n",
       " 'earlier': 630,\n",
       " 'early': 631,\n",
       " 'earns': 632,\n",
       " 'ears': 633,\n",
       " 'earth': 634,\n",
       " 'earthquake': 635,\n",
       " 'ease': 636,\n",
       " 'easily': 637,\n",
       " 'easter': 638,\n",
       " 'easy': 639,\n",
       " 'eat': 640,\n",
       " 'eaten': 641,\n",
       " 'eating': 642,\n",
       " 'eats': 643,\n",
       " 'economic': 644,\n",
       " 'economies': 645,\n",
       " 'education': 646,\n",
       " 'eggs': 647,\n",
       " 'eight': 648,\n",
       " 'eighteen': 649,\n",
       " 'eightthirty': 650,\n",
       " 'elastic': 651,\n",
       " 'elected': 652,\n",
       " 'election': 653,\n",
       " 'electric': 654,\n",
       " 'electricity': 655,\n",
       " 'elephant': 656,\n",
       " 'elephants': 657,\n",
       " 'elevator': 658,\n",
       " 'eleven': 659,\n",
       " 'else': 660,\n",
       " 'embezzled': 661,\n",
       " 'emergency': 662,\n",
       " 'employees': 663,\n",
       " 'employs': 664,\n",
       " 'empty': 665,\n",
       " 'encouraged': 666,\n",
       " 'end': 667,\n",
       " 'enemies': 668,\n",
       " 'enemy': 669,\n",
       " 'engaged': 670,\n",
       " 'engagement': 671,\n",
       " 'engine': 672,\n",
       " 'england': 673,\n",
       " 'english': 674,\n",
       " 'enjoyed': 675,\n",
       " 'enough': 676,\n",
       " 'enter': 677,\n",
       " 'entered': 678,\n",
       " 'entirely': 679,\n",
       " 'entrance': 680,\n",
       " 'envelope': 681,\n",
       " 'enveloped': 682,\n",
       " 'envied': 683,\n",
       " 'environment': 684,\n",
       " 'equal': 685,\n",
       " 'erase': 686,\n",
       " 'escaped': 687,\n",
       " 'essential': 688,\n",
       " 'europe': 689,\n",
       " 'even': 690,\n",
       " 'eventually': 691,\n",
       " 'ever': 692,\n",
       " 'everest': 693,\n",
       " 'every': 694,\n",
       " 'everybody': 695,\n",
       " 'everyone': 696,\n",
       " 'everything': 697,\n",
       " 'everywhere': 698,\n",
       " 'exactly': 699,\n",
       " 'exaggerated': 700,\n",
       " 'exam': 701,\n",
       " 'examination': 702,\n",
       " 'example': 703,\n",
       " 'excellent': 704,\n",
       " 'except': 705,\n",
       " 'exchange': 706,\n",
       " 'exciting': 707,\n",
       " 'excuse': 708,\n",
       " 'exhibited': 709,\n",
       " 'exist': 710,\n",
       " 'expect': 711,\n",
       " 'expectations': 712,\n",
       " 'expenses': 713,\n",
       " 'expensive': 714,\n",
       " 'experiment': 715,\n",
       " 'explain': 716,\n",
       " 'explained': 717,\n",
       " 'exports': 718,\n",
       " 'extend': 719,\n",
       " 'extent': 720,\n",
       " 'eye': 721,\n",
       " 'eyes': 722,\n",
       " 'face': 723,\n",
       " 'facebook': 724,\n",
       " 'faces': 725,\n",
       " 'fact': 726,\n",
       " 'factory': 727,\n",
       " 'facts': 728,\n",
       " 'fail': 729,\n",
       " 'failed': 730,\n",
       " 'failure': 731,\n",
       " 'fainted': 732,\n",
       " 'fair': 733,\n",
       " 'fairies': 734,\n",
       " 'fall': 735,\n",
       " 'fallen': 736,\n",
       " 'families': 737,\n",
       " 'family': 738,\n",
       " 'famous': 739,\n",
       " 'fantastic': 740,\n",
       " 'far': 741,\n",
       " 'farm': 742,\n",
       " 'farmer': 743,\n",
       " 'farther': 744,\n",
       " 'fast': 745,\n",
       " 'faster': 746,\n",
       " 'fat': 747,\n",
       " 'fate': 748,\n",
       " 'father': 749,\n",
       " 'fathers': 750,\n",
       " 'fault': 751,\n",
       " 'faults': 752,\n",
       " 'favor': 753,\n",
       " 'favorite': 754,\n",
       " 'fear': 755,\n",
       " 'feathers': 756,\n",
       " 'feed': 757,\n",
       " 'feel': 758,\n",
       " 'feeling': 759,\n",
       " 'feelings': 760,\n",
       " 'feels': 761,\n",
       " 'feet': 762,\n",
       " 'fell': 763,\n",
       " 'felt': 764,\n",
       " 'fever': 765,\n",
       " 'few': 766,\n",
       " 'fewer': 767,\n",
       " 'field': 768,\n",
       " 'fifteen': 769,\n",
       " 'fifteenth': 770,\n",
       " 'fifth': 771,\n",
       " 'fifty': 772,\n",
       " 'fight': 773,\n",
       " 'fighting': 774,\n",
       " 'figured': 775,\n",
       " 'film': 776,\n",
       " 'find': 777,\n",
       " 'finding': 778,\n",
       " 'fine': 779,\n",
       " 'fined': 780,\n",
       " 'finger': 781,\n",
       " 'finish': 782,\n",
       " 'finished': 783,\n",
       " 'finland': 784,\n",
       " 'fire': 785,\n",
       " 'fired': 786,\n",
       " 'firm': 787,\n",
       " 'first': 788,\n",
       " 'fish': 789,\n",
       " 'fisherman': 790,\n",
       " 'fishing': 791,\n",
       " 'fit': 792,\n",
       " 'five': 793,\n",
       " 'fixed': 794,\n",
       " 'flames': 795,\n",
       " 'flat': 796,\n",
       " 'flew': 797,\n",
       " 'floor': 798,\n",
       " 'flowers': 799,\n",
       " 'fly': 800,\n",
       " 'flying': 801,\n",
       " 'folder': 802,\n",
       " 'follow': 803,\n",
       " 'followed': 804,\n",
       " 'follows': 805,\n",
       " 'fond': 806,\n",
       " 'food': 807,\n",
       " 'foods': 808,\n",
       " 'fool': 809,\n",
       " 'foot': 810,\n",
       " 'football': 811,\n",
       " 'for': 812,\n",
       " 'forehead': 813,\n",
       " 'foreign': 814,\n",
       " 'foreigners': 815,\n",
       " 'forest': 816,\n",
       " 'forget': 817,\n",
       " 'forgetting': 818,\n",
       " 'forgive': 819,\n",
       " 'forgot': 820,\n",
       " 'form': 821,\n",
       " 'fort': 822,\n",
       " 'fortunate': 823,\n",
       " 'fortune': 824,\n",
       " 'forward': 825,\n",
       " 'found': 826,\n",
       " 'four': 827,\n",
       " 'fox': 828,\n",
       " 'france': 829,\n",
       " 'frankly': 830,\n",
       " 'free': 831,\n",
       " 'freedom': 832,\n",
       " 'french': 833,\n",
       " 'frequently': 834,\n",
       " 'fresh': 835,\n",
       " 'friday': 836,\n",
       " 'friend': 837,\n",
       " 'friends': 838,\n",
       " 'from': 839,\n",
       " 'front': 840,\n",
       " 'fulfill': 841,\n",
       " 'full': 842,\n",
       " 'fully': 843,\n",
       " 'fun': 844,\n",
       " 'funds': 845,\n",
       " 'furnished': 846,\n",
       " 'furniture': 847,\n",
       " 'further': 848,\n",
       " 'future': 849,\n",
       " 'gained': 850,\n",
       " 'game': 851,\n",
       " 'garden': 852,\n",
       " 'gas': 853,\n",
       " 'gate': 854,\n",
       " 'gathered': 855,\n",
       " 'gave': 856,\n",
       " 'gazed': 857,\n",
       " 'gentle': 858,\n",
       " 'gentleman': 859,\n",
       " 'gently': 860,\n",
       " 'genuine': 861,\n",
       " 'george': 862,\n",
       " 'german': 863,\n",
       " 'germany': 864,\n",
       " 'get': 865,\n",
       " 'gets': 866,\n",
       " 'getting': 867,\n",
       " 'ghosts': 868,\n",
       " 'girl': 869,\n",
       " 'girls': 870,\n",
       " 'give': 871,\n",
       " 'given': 872,\n",
       " 'gives': 873,\n",
       " 'glad': 874,\n",
       " 'gladly': 875,\n",
       " 'glass': 876,\n",
       " 'glasses': 877,\n",
       " 'glimpse': 878,\n",
       " 'go': 879,\n",
       " 'god': 880,\n",
       " 'goes': 881,\n",
       " 'going': 882,\n",
       " 'gold': 883,\n",
       " 'golf': 884,\n",
       " 'gone': 885,\n",
       " 'good': 886,\n",
       " 'goodbye': 887,\n",
       " 'goodfornothing': 888,\n",
       " 'goods': 889,\n",
       " 'got': 890,\n",
       " 'governed': 891,\n",
       " 'government': 892,\n",
       " 'granddaughter': 893,\n",
       " 'grandfather': 894,\n",
       " 'grandmother': 895,\n",
       " 'granted': 896,\n",
       " 'grapes': 897,\n",
       " 'grateful': 898,\n",
       " 'grave': 899,\n",
       " 'graveyard': 900,\n",
       " 'great': 901,\n",
       " 'greatest': 902,\n",
       " 'greek': 903,\n",
       " 'greeks': 904,\n",
       " 'green': 905,\n",
       " 'group': 906,\n",
       " 'growing': 907,\n",
       " 'growling': 908,\n",
       " 'grown': 909,\n",
       " 'grows': 910,\n",
       " 'guitar': 911,\n",
       " 'gun': 912,\n",
       " 'gut': 913,\n",
       " 'guy': 914,\n",
       " 'guys': 915,\n",
       " 'gym': 916,\n",
       " 'habit': 917,\n",
       " 'had': 918,\n",
       " 'hair': 919,\n",
       " 'half': 920,\n",
       " 'hammered': 921,\n",
       " 'hand': 922,\n",
       " 'handful': 923,\n",
       " 'handle': 924,\n",
       " 'hands': 925,\n",
       " 'hang': 926,\n",
       " 'happen': 927,\n",
       " 'happened': 928,\n",
       " 'happily': 929,\n",
       " 'happiness': 930,\n",
       " 'happy': 931,\n",
       " 'hard': 932,\n",
       " 'harder': 933,\n",
       " 'hardly': 934,\n",
       " 'hardships': 935,\n",
       " 'harm': 936,\n",
       " 'has': 937,\n",
       " 'hasnt': 938,\n",
       " 'haste': 939,\n",
       " 'hat': 940,\n",
       " 'hate': 941,\n",
       " 'hated': 942,\n",
       " 'hates': 943,\n",
       " 'haunted': 944,\n",
       " 'have': 945,\n",
       " 'havent': 946,\n",
       " 'having': 947,\n",
       " 'haze': 948,\n",
       " 'he': 949,\n",
       " 'head': 950,\n",
       " 'headache': 951,\n",
       " 'headaches': 952,\n",
       " 'health': 953,\n",
       " 'healthy': 954,\n",
       " 'heap': 955,\n",
       " 'hear': 956,\n",
       " 'heard': 957,\n",
       " 'heart': 958,\n",
       " 'heat': 959,\n",
       " 'heavily': 960,\n",
       " 'heavy': 961,\n",
       " 'hell': 962,\n",
       " 'hello': 963,\n",
       " 'helmet': 964,\n",
       " 'help': 965,\n",
       " 'helped': 966,\n",
       " 'helps': 967,\n",
       " 'hens': 968,\n",
       " 'her': 969,\n",
       " 'here': 970,\n",
       " 'hers': 971,\n",
       " 'herself': 972,\n",
       " 'hes': 973,\n",
       " 'hesitate': 974,\n",
       " 'hesitated': 975,\n",
       " 'hesitation': 976,\n",
       " 'hid': 977,\n",
       " 'hiding': 978,\n",
       " 'high': 979,\n",
       " 'higher': 980,\n",
       " 'highest': 981,\n",
       " 'him': 982,\n",
       " 'himself': 983,\n",
       " 'his': 984,\n",
       " 'history': 985,\n",
       " 'hit': 986,\n",
       " 'hobby': 987,\n",
       " 'hokkaido': 988,\n",
       " 'hold': 989,\n",
       " 'holding': 990,\n",
       " 'hole': 991,\n",
       " 'holiday': 992,\n",
       " 'home': 993,\n",
       " 'homework': 994,\n",
       " 'honest': 995,\n",
       " 'honesty': 996,\n",
       " 'hope': 997,\n",
       " 'horn': 998,\n",
       " 'horrifyingly': 999,\n",
       " 'horse': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 Python dictionaries - integer index into a word.\n",
    "reverse_input_token_index = dict([(i,ele) for ele,i in input_token_index.items()])\n",
    "reverse_target_token_index = dict([(i,ele) for ele,i in target_token_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Split the data into train and test and define a function which generates the data for train and test in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2496,), (278,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,Y = data['English_Text'],data['Hindi_Text']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.1)\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FINDING MAX LENGTHS OF TARGET AND INPUT SEQUENCES\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad_sequences(preprocess_x)\n",
    "    preprocess_y = pad_sequences(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 22\n",
      "Max French sentence length: 27\n"
     ]
    }
   ],
   "source": [
    "preproc_english_sentences, preproc_hindi_sentences = preprocess(X, Y)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_hindi_sequence_length = preproc_hindi_sentences.shape[1]\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_hindi_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches(X,Y,batch_size=64):\n",
    "    ''' code for generating a batch of data'''\n",
    "    while True:\n",
    "        for j in range(0,len(X),batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_english_sequence_length),dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_hindi_sequence_length),dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_hindi_sequence_length, num_decoder_tokens), dtype='float32')\n",
    "            for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size], Y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i,t] = input_token_index[word] # -----> encoder input sequence\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t<len(target_text.split())-1:\n",
    "                        decoder_input_data[i,t] = target_token_index[word] # ------> decoder input sequence\n",
    "                    if t>0:\n",
    "                        # decoder target ------> one-hot encoded\n",
    "                        # START_ token not included\n",
    "                        # offset by one time step\n",
    "                        decoder_target_data[i,t-1,target_token_index[word]] = 1.\n",
    "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Define the encoder-decoder architecture that has basic LSTM cells at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout,Embedding,InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = tf.keras.layers.Input((None,), name=\"input_enc\")\n",
    "enc_cmb = tf.keras.layers.Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_cmb)\n",
    "# we discard encoder_outputs and keep only the states\n",
    "encoder_states = [state_h,state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# set up the decoder using encoder_states as initial states\n",
    "decoder_inputs = tf.keras.layers.Input((None,), name=\"input_dec\")\n",
    "dec_cmb = tf.keras.layers.Embedding(num_decoder_tokens, latent_dim, mask_zero = True)(decoder_inputs)\n",
    "\n",
    "# setting up decoder to return full output sequences and internal states\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True ,return_state=True)\n",
    "decoder_outputs,_,_ = decoder_lstm(dec_cmb,initial_state=encoder_states)\n",
    "\n",
    "# now the MODEL ---- that will turn 'encoder input data' and 'decoder input data' to 'decoder target data'\n",
    "model = keras.Model([encoder_inputs,decoder_inputs],decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. After training it’s time for prediction on test data. we will have to setup DECODER in TEST MODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the input sequence to get the 'thought vectors'\n",
    "encoder_model = keras.Model(encoder_inputs,encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = tf.keras.layers.Input((latent_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input((latent_dim,))\n",
    "decoder_state_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "\n",
    "\n",
    "# embeddings of the decoder sequence\n",
    "dec_emb2 = tf.keras.layers.Embedding(num_decoder_tokens, latent_dim, mask_zero = True)(decoder_inputs)\n",
    "\n",
    "\n",
    "# to predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2,initial_state=decoder_state_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_dense   = tf.keras.layers.Dense(units=num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = keras.Model(\n",
    "       [decoder_inputs] + decoder_state_inputs,\n",
    "       [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Finally, we will generate the output by defining the following function and later calling it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # encode the input into vectors\n",
    "    states_vectors = encoder_model.predict(input_seq)\n",
    "    # intialize a target sequence of zeros of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # populate the first character of target sequence with the start character\n",
    "    target_seq[0,0] = target_token_index['START_']\n",
    "    \n",
    "    \n",
    "    #loop for a batch of sequences, assuming each batch size=1 for simplicity\n",
    "    stop_condition=False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens,h,c = decoder_model.predict([target_seq]+states_vectors)\n",
    "        \n",
    "        \n",
    "        # sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "        \n",
    "        # exit condition : either hit max length\n",
    "        # or find stop character\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence)>50):\n",
    "            stop_condition= True\n",
    "            \n",
    "        #Update the Target Sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "        \n",
    "        # Update States\n",
    "        states_vectors = [h,c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALLY !!! Results Time - can't wait to see how the machine performs on TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = mini_batches(X_train,Y_train,batch_size=1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: what is the difference between this and that\n",
      "Actual Hindi Translation:  इस और उस में क्या फ़र्क है \n",
      "Predicted Hindi Translation:  दुःखी फ़ुट टूटी फ़ोटोकॉपियर फ़ोटोकॉपियर हमेशा हि\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(training_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_train[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_train[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: nobody could tell what he meant by that\n",
      "Actual Hindi Translation:  कोई नहीं बता सका उसका वह कहने से मतलब क्या था। \n",
      "Predicted Hindi Translation:  लगता मोटे तबाह तबाह उड़ाई। तबाह दौरान बिछाई। सूखा क\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(training_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_train[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_train[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: he looked back and smiled at me\n",
      "Actual Hindi Translation:  उसने पीछे मुड़कर मुझपर मुस्कुराया। \n",
      "Predicted Hindi Translation:  फ़ुट बुलाया। फ़ुट बनी सुनाओ। जेब फ़ोटोकॉपियर बता फ़ोटोकॉ\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(training_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_train[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_train[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: i dont know when she will leave for london\n",
      "Actual Hindi Translation:  मुझे नहीं पता वह लंदन के लिए रवाना कब होएगी। \n",
      "Predicted Hindi Translation:  आऊँगा। लगता मोटे महाराष्ट्र चेतावनी जाए लेखक लेखक अक्ख\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(training_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_train[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_train[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: she was brought up by her grandmother\n",
      "Actual Hindi Translation:  उसकी दादी ने उसे पालपोस कर बड़ा किया था। \n",
      "Predicted Hindi Translation:  पालनी खो खो जाएँगे। जाएँगे। जाएँगे। जाएँगे। मिल\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(training_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_train[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_train[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "well the model is not at all performing well on the training data, a detailed scrutiny is needed,\n",
    "probably the dataset is too small and also the model is a very simple one and the vocabulary is generated\n",
    "from the training set itself which is very small (can consider taking from the internet) ,\n",
    "all these changes can help it perform well.\n",
    "\n",
    "what matters is we tried !!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = mini_batches(X_test,Y_test,batch_size=1)\n",
    "k=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: this car was made in japan\n",
      "Actual Hindi Translation:  यह गाड़ी जापान में बनी थी। \n",
      "Predicted Hindi Translation:  वाह पैक खींची। तौलिए निभता जानते तुरन्त तुरन्त \n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(test_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_test[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_test[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: i think there has been some misunderstanding here\n",
      "Actual Hindi Translation:  मुझे लगता है कि यहाँ कुछ ग़लतफ़ैमी हुई है। \n",
      "Predicted Hindi Translation:  आए रहे। रहे। अक्खड़पन इक्कीस रोना पार चढ़ा। बी\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(test_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_test[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_test[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Input Sentence: is your father a teacher\n",
      "Actual Hindi Translation:  क्या आपके पापा टीचर हैं \n",
      "Predicted Hindi Translation:  शब्दकोष ख़राब नानी निशान हमे मोटे आजकल अकल संदेश शादी\n"
     ]
    }
   ],
   "source": [
    "k+=1\n",
    "(input_seq,actual_output),_ = next(test_batch)\n",
    "decoded_sentence = decode_sequence(input_seq)\n",
    "print('English Input Sentence:',X_test[k:k+1].values[0])\n",
    "print(\"Actual Hindi Translation:\",Y_test[k:k+1].values[0][6:-4])\n",
    "print(\"Predicted Hindi Translation:\",decoded_sentence[:-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----------------------------------- END ---------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
