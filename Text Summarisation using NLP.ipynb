{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Customer reviews are long and descriptive. Analyzing these reviews manually, is really time-consuming. Natural Language Processing comes to our rescue to generate a summary for long reviews.\n",
    "\n",
    "Our objective here is to generate a summary for the Amazon Fine Food reviews using the abstraction-based approach \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In **abstraction based approach** we generate new sentences from the original text in contrast to the extractive approach where the sentences are a part of original text. The sentences generated through abstractive summarization might not be present in the original text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Text Summarization in Python using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting attention\n",
      "  Downloading attention-4.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /Applications/anaconda3/lib/python3.7/site-packages (from attention) (1.19.5)\n",
      "Requirement already satisfied: tensorflow>=2.1 in /Applications/anaconda3/lib/python3.7/site-packages (from attention) (2.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (3.15.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (0.36.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.4.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.37.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (0.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.1->attention) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (56.0.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (3.3.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Applications/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Applications/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Applications/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Applications/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (4.0.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Applications/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Applications/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Applications/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Applications/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.1->attention) (3.7.4.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: attention\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Successfully installed attention-4.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting keras-self-attention\n",
      "  Downloading keras-self-attention-0.49.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: numpy in /Applications/anaconda3/lib/python3.7/site-packages (from keras-self-attention) (1.19.5)\n",
      "Requirement already satisfied: Keras in /Applications/anaconda3/lib/python3.7/site-packages (from keras-self-attention) (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /Applications/anaconda3/lib/python3.7/site-packages (from Keras->keras-self-attention) (1.4.1)\n",
      "Requirement already satisfied: h5py in /Applications/anaconda3/lib/python3.7/site-packages (from Keras->keras-self-attention) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /Applications/anaconda3/lib/python3.7/site-packages (from Keras->keras-self-attention) (5.1.2)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.7/site-packages (from h5py->Keras->keras-self-attention) (1.15.0)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.49.0-py3-none-any.whl size=19467 sha256=fe827b85580eb6c72ec169aee9bd593bfdd6a33a1667b1fe2dcf986e0696f28d\n",
      "  Stored in directory: /Users/meghanaanantaneni/Library/Caches/pip/wheels/72/f2/4d/f9a9d35bc8907948afb906414cf4196d8e455bdc9fb94b898e\n",
      "Successfully built keras-self-attention\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: keras-self-attention\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Successfully installed keras-self-attention-0.49.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/Applications/anaconda3/lib/python3.7/site-packages)\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AttentionLayer' from 'attention' (/Applications/anaconda3/lib/python3.7/site-packages/attention/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-21847fb92901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AttentionLayer' from 'attention' (/Applications/anaconda3/lib/python3.7/site-packages/attention/__init__.py)"
     ]
    }
   ],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import re           \n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords   \n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Reviews.csv\",nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates and Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)   #dropping na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we will drop all the unwanted symbols, characters, etc. \n",
    "from the text that do not affect the objective of our problem.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary that we will use for expanding the contractions:\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labr...\n",
       "1             Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
       "2    This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with ...\n",
       "3    If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The fl...\n",
       "4                                                               Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
       "5    I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there wa...\n",
       "6    This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Cleaning\n",
    "# examine the reviews to get an idea of text preprocessing:\n",
    "data['Text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below preprocessing steps will be performed -\n",
    "\n",
    "1. Convert everything to lowercase\n",
    "\n",
    "\n",
    "2. Remove HTML tags\n",
    "\n",
    "\n",
    "3. Contraction mapping\n",
    "\n",
    "\n",
    "4. Remove (‘s)\n",
    "\n",
    "\n",
    "5. Remove any text inside the parenthesis ( )\n",
    "\n",
    "\n",
    "6. Eliminate punctuations and special characters\n",
    "\n",
    "\n",
    "7. Remove stopwords\n",
    "\n",
    "\n",
    "8. Remove short words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "cleaned_text = []\n",
    "for t in data['Text']:\n",
    "    cleaned_text.append(text_cleaner(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Good Quality Dog Food\n",
       "1                                Not as Advertised\n",
       "2                            \"Delight\" says it all\n",
       "3                                   Cough Medicine\n",
       "4                                      Great taffy\n",
       "5                                       Nice Taffy\n",
       "6    Great!  Just as good as the expensive brands!\n",
       "7                           Wonderful, tasty taffy\n",
       "8                                       Yay Barley\n",
       "9                                 Healthy Dog Food\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary Cleaning\n",
    "\n",
    "# examine reviews for idea of the preprocessing for the summary column\n",
    "\n",
    "data['Summary'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cleaner(text):\n",
    "    newString = re.sub('\"','', text)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    newString = newString.lower()\n",
    "    tokens=newString.split()\n",
    "    newString=''\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                 \n",
    "            newString=newString+i+' '  \n",
    "    return newString\n",
    "\n",
    "#Call the above function\n",
    "cleaned_summary = []\n",
    "for t in data['Summary']:\n",
    "    cleaned_summary.append(summary_cleaner(t))\n",
    "\n",
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary\n",
    "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add START and END special tokens at the beginning and end of the summary:\n",
    "data['cleaned_summary'] = data['cleaned_summary'].apply(lambda x : '_START_ '+ x + ' _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: right mostly sprouting cats eat grass love rotate around wheatgrass rye\n",
      "Summary: _START_ yay barley  _END_\n",
      "\n",
      "\n",
      "Review: healthy dog food good digestion also good small puppies dog eats required amount every feeding\n",
      "Summary: _START_ healthy dog food  _END_\n",
      "\n",
      "\n",
      "Review: know cactus tequila unique combination ingredients flavour hot sauce makes one kind picked bottle trip brought back home totally blown away realized simply could find anywhere city bummed magic internet case sauce ecstatic love hot sauce mean really love hot sauce want sauce tastelessly burns throat grab bottle tequila picante gourmet inclan realize taste never want use sauce thank personal incredible service\n",
      "Summary: _START_ the best hot sauce in the world  _END_\n",
      "\n",
      "\n",
      "Review: one boys needed lose weight put food floor chubby guy protein rich product food higher skinny boy jump higher food sits going stale really food chubby boy losing ounce week\n",
      "Summary: _START_ my cats love this diet food better than their regular food  _END_\n",
      "\n",
      "\n",
      "Review: cats happily eating felidae platinum two years got new bag shape food different tried new food first put bowls bowls sit full kitties touch food noticed similar reviews related formula changes past unfortunately need find new food cats eat\n",
      "Summary: _START_ my cats are not fans of the new food  _END_\n",
      "\n",
      "\n",
      "Review: good flavor came securely packed fresh delicious love twizzlers\n",
      "Summary: _START_ fresh and greasy  _END_\n",
      "\n",
      "\n",
      "Review: strawberry twizzlers guilty pleasure yummy six pounds around son\n",
      "Summary: _START_ strawberry twizzlers yummy  _END_\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look at the in between reviews and their summary:\n",
    "\n",
    "for i in range(8,15):\n",
    "    print(\"Review:\",data['cleaned_text'][i])\n",
    "    print(\"Summary:\",data['cleaned_summary'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the distribution of the sequences\n",
    "\n",
    "Analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text,which will help us decide the maximum length of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can fix the maximum length of the reviews to 80 as that's the majority review length and\n",
    "can set the maximum summary length to 10\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=80 \n",
    "max_len_summary=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the dataset into a training and validation set. \n",
    "\n",
    "We’ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(data['cleaned_text'],data['cleaned_summary'],test_size=0.1,random_state=0,shuffle=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Tokenizer -  to build the vocabulary and convert a word sequence to an integer sequence. \n",
    "\n",
    "Building tokenizers for text and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Tokenizer\n",
    "#tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary Tokenizer\n",
    "#tokenizer for summary on training data \n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert summary sequences into integer sequences\n",
    "y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
    "y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
    "\n",
    "y_voc_size  =   len(y_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building -\n",
    "\n",
    "\n",
    "\n",
    "1. When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep.\n",
    "\n",
    "\n",
    "2. When return state = True, LSTM produces the hidden state and cell state of the last timestep only.\n",
    "\n",
    "\n",
    "3. Initial State: This is used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "\n",
    "4. Stacked LSTM: Stacked LSTM has multiple layers of LSTM stacked on top of each other. This leads to a better representation of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we are building a 3 stacked LSTM for the encoder\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-11983d50610d>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-11983d50610d>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    Attention layer attn_layer = Attention(name='attention_layer')\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session() \n",
    "latent_dim = 500 \n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Set up the decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "Attention layer attn_layer = Attention(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---------------- Installation issues------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####need to look into this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
